{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShSWYFSlENxq"
      },
      "source": [
        "# Simple Character LSTM\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPTxJL3qENxu",
        "outputId": "ff06f226-90ea-4702-e115-6cfd542f6ada",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Preprocessing string data\n",
        "# alphabet(0-25), space(26),..., start, end\n",
        "\n",
        "string = \"hello pytorch. how long can a rnn cell remember? show me your limit!\"\n",
        "chars = \"abcdefghijklmnopqrstuvwxyz ?!.,:;01\"\n",
        "char_list = [i for i in chars]\n",
        "char_len = len(char_list)\n",
        "\n",
        "char_len"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-xqHFGdENx0"
      },
      "source": [
        "# String to onehot vector\n",
        "# a -> [1 0 0 ... 0 0]\n",
        "\n",
        "\n",
        "\n",
        "def string_to_onehot(string):\n",
        "    start = np.zeros(shape=char_len ,dtype=int)\n",
        "    end = np.zeros(shape=char_len ,dtype=int)\n",
        "\n",
        "    start[-2] = 1\n",
        "    end[-1] = 1\n",
        "\n",
        "    for i in string:\n",
        "        idx = char_list.index(i)\n",
        "        zero = np.zeros(shape=char_len ,dtype=int)\n",
        "        zero[idx]=1\n",
        "        start = np.vstack([start,zero])\n",
        "    output = np.vstack([start,end])\n",
        "    return output"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_gqctcyENx3"
      },
      "source": [
        "# Onehot vector to word\n",
        "# [1 0 0 ... 0 0] -> a\n",
        "\n",
        "def onehot_to_word(onehot_1):\n",
        "    onehot = torch.Tensor.numpy(onehot_1)\n",
        "    return char_list[onehot.argmax()]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjb-I6DlENx6",
        "outputId": "05e54e82-559b-46b6-f7a3-e96f38a0cc13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 하이퍼파라미터 설정\n",
        "# 문자열을 단어 하나씩 잘러서 사용하는걸로 구현해서 batch_size 1로 고정입니다.\n",
        "# batch_size가 1보다 큰 경우는 다음 실습코드에 있습니다.\n",
        "\n",
        "batch_size = 1 #문장이 하나라서. 문자 여러개면 배치 사이즈 여러개로.\n",
        "\n",
        "# seq_len는 바꿔도 학습은 되지만 테스트시 편의성을 위해 1로 설정했습니다.\n",
        "seq_len = 1 #각 입력 독립적으로 처리, 2로 해주면 2개의 입력 한번에 처리.\n",
        "\n",
        "# num_layers는 입력 형식에만 맞게 형태를 바꿔주면 됩니다.\n",
        "num_layers = 3\n",
        "input_size = char_len # 35개의 문자.\n",
        "hidden_size = 35\n",
        "lr = 0.01\n",
        "num_epochs = 1000\n",
        "\n",
        "one_hot = torch.from_numpy(string_to_onehot(string)).type_as(torch.FloatTensor())\n",
        "\n",
        "print(one_hot.size()) #원래 문장 길이는 68, start랑 end가 합쳐져서 70 됨."
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([70, 35])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGTtv6T5ENx9"
      },
      "source": [
        "# RNN with 1 hidden layer\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size,num_layers):\n",
        "        super(RNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size,hidden_size,num_layers) #파이토치안에서 그냥 LSTM 함수 만들어놓음. 참 쉽죠! 그죠! 으흐흐ㅡ흐흐하아하하ㅏㅏ..\n",
        "\n",
        "    def forward(self,input_,hidden,cell):\n",
        "        output,(hidden,cell) = self.lstm(input_,(hidden,cell))\n",
        "        return output,hidden,cell\n",
        "\n",
        "    def init_hidden_cell(self):\n",
        "        hidden = torch.zeros(num_layers,batch_size,hidden_size)\n",
        "        cell = torch.zeros(num_layers,batch_size,hidden_size)\n",
        "        return hidden,cell\n",
        "\n",
        "rnn = RNN(input_size,hidden_size, num_layers) # RNN계층 생성."
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Gt2qWjRENyA"
      },
      "source": [
        "# Loss function & Optimizer\n",
        "loss_func = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(rnn.parameters(), lr=lr)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0xJsfTQENyD"
      },
      "source": [
        "j=0\n",
        "input_data = one_hot[j:j+seq_len].view(seq_len, batch_size, input_size) #seq_len값은 아까 1로.\n",
        "#print(input_data.size())\n",
        "\n",
        "hidden,cell = rnn.init_hidden_cell()\n",
        "#print(hidden.size(),cell.size())\n",
        "\n",
        "output, hidden,cell = rnn(input_data,hidden,cell)\n",
        "#print(output.size(),hidden.size(),cell.size())"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92Vh1bteEXKu",
        "outputId": "b40d81e1-5308-45be-8834-3b185e8c346f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "unroll_len = one_hot.size()[0] // seq_len -1 #맨 마지막 글자 뺸 개수 만큼.\n",
        "for i in range(num_epochs):\n",
        "    hidden,cell = rnn.init_hidden_cell()\n",
        "\n",
        "    loss = 0\n",
        "    for j in range(unroll_len): #총 69번.\n",
        "        input_data = one_hot[j:j+seq_len].view(seq_len, batch_size, input_size) #pytorch란 문자면, p 그래서 그냥 j\n",
        "        label = one_hot[j+1:j+seq_len+1].view(seq_len, batch_size, input_size) # 그 다음 y인데, 그래서 j+1(한칸 더 간거.)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output, hidden, cell = rnn(input_data,hidden,cell)\n",
        "        loss += loss_func(output.view(1,-1), label.view(1,-1))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if i%10 ==0:\n",
        "        print(loss)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.4432, grad_fn=<AddBackward0>)\n",
            "tensor(1.8139, grad_fn=<AddBackward0>)\n",
            "tensor(1.7405, grad_fn=<AddBackward0>)\n",
            "tensor(1.5782, grad_fn=<AddBackward0>)\n",
            "tensor(1.3532, grad_fn=<AddBackward0>)\n",
            "tensor(1.0168, grad_fn=<AddBackward0>)\n",
            "tensor(0.7045, grad_fn=<AddBackward0>)\n",
            "tensor(0.4263, grad_fn=<AddBackward0>)\n",
            "tensor(0.2377, grad_fn=<AddBackward0>)\n",
            "tensor(0.1467, grad_fn=<AddBackward0>)\n",
            "tensor(0.0851, grad_fn=<AddBackward0>)\n",
            "tensor(0.0533, grad_fn=<AddBackward0>)\n",
            "tensor(0.0376, grad_fn=<AddBackward0>)\n",
            "tensor(0.0289, grad_fn=<AddBackward0>)\n",
            "tensor(0.0236, grad_fn=<AddBackward0>)\n",
            "tensor(0.0196, grad_fn=<AddBackward0>)\n",
            "tensor(0.0170, grad_fn=<AddBackward0>)\n",
            "tensor(0.0153, grad_fn=<AddBackward0>)\n",
            "tensor(0.0170, grad_fn=<AddBackward0>)\n",
            "tensor(0.0145, grad_fn=<AddBackward0>)\n",
            "tensor(0.0129, grad_fn=<AddBackward0>)\n",
            "tensor(0.0122, grad_fn=<AddBackward0>)\n",
            "tensor(0.0115, grad_fn=<AddBackward0>)\n",
            "tensor(0.0107, grad_fn=<AddBackward0>)\n",
            "tensor(0.0098, grad_fn=<AddBackward0>)\n",
            "tensor(0.0092, grad_fn=<AddBackward0>)\n",
            "tensor(0.0084, grad_fn=<AddBackward0>)\n",
            "tensor(0.0106, grad_fn=<AddBackward0>)\n",
            "tensor(0.0079, grad_fn=<AddBackward0>)\n",
            "tensor(0.0075, grad_fn=<AddBackward0>)\n",
            "tensor(0.0071, grad_fn=<AddBackward0>)\n",
            "tensor(0.0068, grad_fn=<AddBackward0>)\n",
            "tensor(0.0066, grad_fn=<AddBackward0>)\n",
            "tensor(0.0064, grad_fn=<AddBackward0>)\n",
            "tensor(0.0061, grad_fn=<AddBackward0>)\n",
            "tensor(0.0055, grad_fn=<AddBackward0>)\n",
            "tensor(0.0051, grad_fn=<AddBackward0>)\n",
            "tensor(0.0049, grad_fn=<AddBackward0>)\n",
            "tensor(0.0048, grad_fn=<AddBackward0>)\n",
            "tensor(0.0047, grad_fn=<AddBackward0>)\n",
            "tensor(0.0046, grad_fn=<AddBackward0>)\n",
            "tensor(0.0045, grad_fn=<AddBackward0>)\n",
            "tensor(0.0044, grad_fn=<AddBackward0>)\n",
            "tensor(0.0044, grad_fn=<AddBackward0>)\n",
            "tensor(0.0043, grad_fn=<AddBackward0>)\n",
            "tensor(0.0044, grad_fn=<AddBackward0>)\n",
            "tensor(0.0046, grad_fn=<AddBackward0>)\n",
            "tensor(0.0043, grad_fn=<AddBackward0>)\n",
            "tensor(0.0042, grad_fn=<AddBackward0>)\n",
            "tensor(0.0042, grad_fn=<AddBackward0>)\n",
            "tensor(0.0041, grad_fn=<AddBackward0>)\n",
            "tensor(0.0041, grad_fn=<AddBackward0>)\n",
            "tensor(0.0040, grad_fn=<AddBackward0>)\n",
            "tensor(0.0040, grad_fn=<AddBackward0>)\n",
            "tensor(0.0040, grad_fn=<AddBackward0>)\n",
            "tensor(0.0040, grad_fn=<AddBackward0>)\n",
            "tensor(0.0040, grad_fn=<AddBackward0>)\n",
            "tensor(0.0039, grad_fn=<AddBackward0>)\n",
            "tensor(0.0039, grad_fn=<AddBackward0>)\n",
            "tensor(0.0039, grad_fn=<AddBackward0>)\n",
            "tensor(0.0039, grad_fn=<AddBackward0>)\n",
            "tensor(0.0038, grad_fn=<AddBackward0>)\n",
            "tensor(0.0038, grad_fn=<AddBackward0>)\n",
            "tensor(0.0038, grad_fn=<AddBackward0>)\n",
            "tensor(0.0038, grad_fn=<AddBackward0>)\n",
            "tensor(0.0040, grad_fn=<AddBackward0>)\n",
            "tensor(0.0039, grad_fn=<AddBackward0>)\n",
            "tensor(0.0038, grad_fn=<AddBackward0>)\n",
            "tensor(0.0038, grad_fn=<AddBackward0>)\n",
            "tensor(0.0037, grad_fn=<AddBackward0>)\n",
            "tensor(0.0037, grad_fn=<AddBackward0>)\n",
            "tensor(0.0037, grad_fn=<AddBackward0>)\n",
            "tensor(0.0037, grad_fn=<AddBackward0>)\n",
            "tensor(0.0037, grad_fn=<AddBackward0>)\n",
            "tensor(0.0036, grad_fn=<AddBackward0>)\n",
            "tensor(0.0036, grad_fn=<AddBackward0>)\n",
            "tensor(0.0037, grad_fn=<AddBackward0>)\n",
            "tensor(0.0039, grad_fn=<AddBackward0>)\n",
            "tensor(0.0037, grad_fn=<AddBackward0>)\n",
            "tensor(0.0036, grad_fn=<AddBackward0>)\n",
            "tensor(0.0036, grad_fn=<AddBackward0>)\n",
            "tensor(0.0036, grad_fn=<AddBackward0>)\n",
            "tensor(0.0036, grad_fn=<AddBackward0>)\n",
            "tensor(0.0035, grad_fn=<AddBackward0>)\n",
            "tensor(0.0035, grad_fn=<AddBackward0>)\n",
            "tensor(0.0035, grad_fn=<AddBackward0>)\n",
            "tensor(0.0035, grad_fn=<AddBackward0>)\n",
            "tensor(0.0035, grad_fn=<AddBackward0>)\n",
            "tensor(0.0035, grad_fn=<AddBackward0>)\n",
            "tensor(0.0035, grad_fn=<AddBackward0>)\n",
            "tensor(0.0038, grad_fn=<AddBackward0>)\n",
            "tensor(0.0035, grad_fn=<AddBackward0>)\n",
            "tensor(0.0035, grad_fn=<AddBackward0>)\n",
            "tensor(0.0035, grad_fn=<AddBackward0>)\n",
            "tensor(0.0035, grad_fn=<AddBackward0>)\n",
            "tensor(0.0035, grad_fn=<AddBackward0>)\n",
            "tensor(0.0034, grad_fn=<AddBackward0>)\n",
            "tensor(0.0034, grad_fn=<AddBackward0>)\n",
            "tensor(0.0034, grad_fn=<AddBackward0>)\n",
            "tensor(0.0035, grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VJSE-SlENyH",
        "outputId": "18645f44-e7ef-4f71-ad64-886180187492",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "hidden,cell = rnn.init_hidden_cell()\n",
        "\n",
        "for j in range(unroll_len-1):\n",
        "    input_data = one_hot[j:j+1].view(1,batch_size,hidden_size)\n",
        "    label = one_hot[j+1:j+1+1].view(1,batch_size,hidden_size)\n",
        "\n",
        "    output, hidden, cell = rnn(input_data,hidden,cell)\n",
        "    print(onehot_to_word(output.data),end=\"\")\n",
        "\n",
        "    #밑에 출력결과 보면, 같은 띄어쓰기인데, 예측값이 달라지는 것은, 기억력이 좋다는 것.(LSTM 좋다.)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello pytorch. how long can a rnn cell remember? show me your limit!"
          ]
        }
      ]
    }
  ]
}